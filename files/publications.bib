@inproceedings{Forest2018,
abstract = {A major application of data analytics for aircraft engine manufacturers is engine health monitoring, which consists in improving availability and operation of engines by leveraging operational data and past events. Traditional tools can no longer handle the increasing volume and velocity of data collected on modern aircraft. We propose a generic and scalable pipeline for large-scale analytics of operational data from a recent type of aircraft engine, oriented towards health monitoring applications. Based on Hadoop and Spark, our approach enables domain experts to scale their algorithms and extract features from tens of thousands of flights stored on a cluster. All computations are performed using the Spark framework, however custom functions and algorithms can be integrated without knowledge of distributed programming. Unsupervised learning algorithms are integrated for clustering and dimensionality reduction of the flight features, in order to allow efficient visualization and interpretation through a dedicated web application. The use case guiding our work is a methodology for engine fleet monitoring with a self-organizing map. Finally, this pipeline is meant to be end-to-end, fully customizable and ready for use in an industrial setting.},
author = {Forest, Florent and Lacaille, J{\'{e}}r{\^{o}}me and Lebbah, Mustapha and Azzag, Hanane},
booktitle = {IEEE International Conference on Big Data 2018},
doi = {10.1109/BigData.2018.8622297},
isbn = {9781538650356},
keywords = {big data,aircraft engine,aviation,generic,hadoop,health monitoring,scalable,spark},
pages = {1--7},
title = {{A Generic and Scalable Pipeline for Large-Scale Analytics of Continuous Aircraft Engine Data}},
year = {2018},
url_Link = {https://ieeexplore.ieee.org/document/8622297},
url_Paper = {IEEEBigData-2018-ForestLacailleLebbahAzzag-full-paper.pdf}
}
@inproceedings{Forest2019a,
abstract = {Recent research has demonstrated how deep neural networks are able to learn representations to improve data clustering. By considering representation learning and clustering as a joint task, models learn clustering-friendly spaces and achieve superior performance, com- pared with standard two-stage approaches where dimensionality reduc- tion and clustering are performed separately. We extend this idea to topology-preserving clustering models, known as self-organizing maps (SOM). First, we present the Deep Embedded Self-Organizing Map (DE- SOM), a model composed of a fully-connected autoencoder and a custom SOM layer, where the SOM code vectors are learnt jointly with the au- toencoder weights. Then, we show that this generic architecture can be extended to image and sequence data by using convolutional and recur- rent architectures, and present variants of these models. First results demonstrate advantages of the DESOM architecture in terms of cluster- ing performance, visualization and training time.},
author = {Forest, Florent and Lebbah, Mustapha and Azzag, Hanane and Lacaille, J{\'{e}}r{\^{o}}me},
booktitle = {Workshop on Learning Data Representations for Clustering (LDRC), PAKDD 2019},
doi = {10.1007/978-3-030-26142-9_10},
keywords = {autoencoder,clustering,deep learning,representation learning,self-organizing map},
pages = {1--12},
title = {{Deep Architectures for Joint Clustering and Visualization with Self-Organizing Maps}},
year = {2019},
url_Link = {https://link.springer.com/chapter/10.1007/978-3-030-26142-9_10},
url_Paper = {LDRC-2019-DeepArchitecturesJointClusteringVisualization-full-paper.pdf}
}
@inproceedings{Forest2019,
abstract = {In the wake of recent advances in joint clustering and deep learning, we introduce the Deep Embedded Self-Organizing Map, a model that jointly learns representations and the code vectors of a self-organizing map. Our model is composed of an autoencoder and a custom SOM layer that are optimized in a joint training procedure, motivated by the idea that the SOM prior could help learning SOM-friendly representations. We eval- uate SOM-based models in terms of clustering quality and unsupervised clustering accuracy, and study the benefits of joint training.},
author = {Forest, Florent and Lebbah, Mustapha and Azzag, Hanane and Lacaille, J{\'{e}}r{\^{o}}me},
booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2019)},
keywords = {autoencoder,clustering,deep learning,representation learning,self-organizing map},
pages = {1--6},
title = {{Deep Embedded SOM: Joint Representation Learning and Self-Organization}},
year = {2019},
url_Link = {https://www.i6doc.com/en/book/?gcoi=28001100931280},
url_Paper = {ESANN-2019-DeepEmbeddedSOM-full-paper.pdf},
url_Slides = {ESANN-2019-DeepEmbeddedSOM-pres.pdf},
url_Code = {https://github.com/FlorentF9/DESOM}
}
@misc{Forest2019b,
author = {Forest, Florent},
title = {{Poster: Clustering de donn{\'{e}}es massives - Analyse de donn{\'{e}}es de turbofans}},
year = {2019},
source_type = {poster},
url_Paper = {poster-journeeED-2019.pdf}
}
@inproceedings{Forest2020,
abstract = {Dans la lign{\'{e}}e des r{\'{e}}centes avanc{\'{e}}es en apprentissage profond de repr{\'{e}}sentations pour le clustering, ce travail (pr{\'{e}}c{\'{e}}demment publi{\'{e}} en anglais) pr{\'{e}}sente le mod{\`{e}}le DESOM (Deep Embedded SOM), combinant l'apprentisssage non supervis{\'{e}} de repr{\'{e}}sentations et d'une carte auto-organis{\'{e}}e de Kohonen (SOM). Le mod{\`{e}}le, compos{\'{e}} d'un auto-encodeur et d'une couche SOM, est optimis{\'{e}} conjointement, an de r{\'{e}}gulariser l'espace latent et am{\'{e}}liorer la performance de la carte SOM. Nous {\'{e}}valuons les performances de classification et de visualisation ainsi que les b{\'{e}}n{\'{e}}fices de l'apprentissage joint. Mots-clef : carte auto-organis{\'{e}}e, clustering, apprentissage profond, auto-encodeur.},
author = {Forest, Florent and Lebbah, Mustapha and Azzag, Hanene and Lacaille, J{\'{e}}r{\^{o}}me},
booktitle = {CAp2020: Conf{\'{e}}rence d'Apprentissage},
keywords = {autoencoder,clustering,deep learning,self-organizing map},
title = {{Carte SOM profonde : Apprentissage joint de repr{\'{e}}sentations et auto-organisation}},
url_Link = {https://hal.archives-ouvertes.fr/hal-02859997},
url_Paper = {https://hal.archives-ouvertes.fr/hal-02859997/document},
year = {2020}
}
@unpublished{Mourer2020,
abstract = {Model selection is a major challenge in non-parametric clustering. There is no universally admitted way to evaluate clustering results for the obvious reason that there is no ground truth against which results could be tested, as in supervised learning. The difficulty to find a universal evaluation criterion is a direct consequence of the fundamentally ill-defined objective of clustering. In this perspective, clustering stability has emerged as a natural and model-agnostic principle: an algorithm should find stable structures in the data. If data sets are repeatedly sampled from the same underlying distribution, an algorithm should find similar partitions. However, it turns out that stability alone is not a well-suited tool to determine the number of clusters. For instance, it is unable to detect if the number of clusters is too small. We propose a new principle for clustering validation: a good clustering should be stable, and within each cluster, there should exist no stable partition. This principle leads to a novel internal clustering validity criterion based on between-cluster and within-cluster stability, overcoming limitations of previous stability-based methods. We empirically show the superior ability of additive noise to discover structures, compared with sampling-based perturbation. We demonstrate the effectiveness of our method for selecting the number of clusters through a large number of experiments and compare it with existing evaluation methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:2006.08530v1},
author = {Mourer, Alex and Forest, Florent and Lebbah, Mustapha and Azzag, Hanane and Lacaille, J{\'{e}}r{\^{o}}me},
eprint = {arXiv:2006.08530v1},
keywords = {Validity index,clustering,model selection,stability analysis},
title = {{Selecting the Number of Clusters K with a Stability Trade-off : an Internal Validation Criterion}},
url_Link = {https://arxiv.org/abs/2006.08530},
url_Paper = {https://arxiv.org/pdf/2006.08530.pdf},
year = {2020}
}
@article{Knodlseder2016,
abstract = {The field of gamma-ray astronomy has seen important progress during the last decade, yet to date no common software framework has been developed for the scientific analysis of gamma-ray telescope data. We propose to fill this gap by means of the GammaLib software, a generic library that we have developed to support the analysis of gamma-ray event data. GammaLib was written in C++ and all functionality is available in Python through an extension module. Based on this framework we have developed the ctools software package, a suite of software tools that enables flexible workflows to be built for the analysis of Imaging Air Cherenkov Telescope event data. The ctools are inspired by science analysis software available for existing high-energy astronomy instruments, and they follow the modular ftools model developed by the High Energy Astrophysics Science Archive Research Center. The ctools were written in Python and C++, and can be either used from the command line via shell scripts or directly from Python. In this paper we present the GammaLib and ctools software versions 1.0 that were released at the end of 2015. GammaLib and ctools are ready for the science analysis of Imaging Air Cherenkov Telescope event data, and also support the analysis of Fermi-LAT data and the exploitation of the COMPTEL legacy data archive. We propose using ctools as the science tools software for the Cherenkov Telescope Array Observatory.},
archivePrefix = {arXiv},
arxivId = {1606.00393},
author = {Kn{\"{o}}dlseder, J. and Mayer, M. and Deil, C. and Cayrou, J. B. and Owen, E. and Kelley-Hoskins, N. and Lu, C. C. and Buehler, R. and Forest, F. and Louge, T. and Siejkowski, H. and Kosack, K. and Gerard, L. and Schulz, A. and Martin, P. and Sanchez, D. and Ohm, S. and Hassan, T. and Brau-Nogu{\'{e}}, S.},
doi = {10.1051/0004-6361/201628822},
eprint = {1606.00393},
issn = {14320746},
journal = {Astronomy and Astrophysics},
keywords = {data analysis,virtual observatory tools},
pages = {1--19},
title = {{GammaLib and ctools: A software framework for the analysis of astronomical gamma-ray data}},
volume = {593},
year = {2016},
url_Link = {https://www.aanda.org/articles/aa/abs/2016/09/aa28822-16/aa28822-16.html},
url_Paper = {https://www.aanda.org/articles/aa/pdf/2016/09/aa28822-16.pdf}
}
